<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Q-DRIVE: Quadrupedal Deep Reinforcement Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&family=Merriweather:wght@700&display=swap" rel="stylesheet">

  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Open Sans', sans-serif;
      color: #ffffff;
      background-color: #1a1a1a;
      line-height: 1.5;
    }
    
    a {
      color: #3498db;
      text-decoration: none;
    }
    
    a:hover {
      text-decoration: underline;
      color: #2980b9;
    }

    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 20px;
      text-align: center;
    }

    .title-banner {
      background: linear-gradient(rgba(0, 0, 0, 0.7), rgba(0, 0, 0, 0.7)),
                  url("./public/test.png") no-repeat center center;
      background-size: cover;
      min-height: 400px;
      display: flex;
      justify-content: center;
      align-items: center;
      padding: 40px;
      position: relative;
    }

    .main-title {
      font-family: 'Merriweather', serif;
      font-size: 2.5rem;
      font-weight: 700;
      color: #ffffff;
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
      line-height: 1.3;
      margin-bottom: 0;
      text-align: center;
    }

    .authors {
      font-size: 1.1rem;
      margin: 40px 0;
      color: #cccccc;
    }

    .authors span {
      display: inline-block;
      margin: 0 5px;
      line-height: 1.4;
    }

    .affiliation-note {
      font-size: 0.95rem;
      color: #999999;
      display: block;
      margin-top: 8px;
    }

    .icons-row {
      display: flex;
      justify-content: center;
      align-items: center;
      margin: 50px 0;
      gap: 30px;
    }

    .icon-wrapper {
      margin: 0 15px;
      text-align: center;
    }

    .icon-wrapper img {
      width: 36px;
      height: 36px;
      display: block;
      margin: 0 auto 5px auto;
      filter: invert(1);
    }

    .icon-wrapper a {
      color: #ffffff;
      font-weight: 600;
      font-size: 1rem;
    }

    .section-title {
      font-size: 1.75rem;
      font-weight: 600;
      margin: 40px 0 20px 0;
      color: #3498db;
    }

    .content-text {
      color: #cccccc;
      text-align: left;
      line-height: 1.6;
      margin-bottom: 20px;
    }

    .images-section {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 20px;
      margin: 50px 0;
    }

    .images-section img {
      max-width: 300px;
      width: 100%;
      height: auto;
      display: block;
      border: 1px solid #333333;
      border-radius: 4px;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      height: 0;
      overflow: hidden;
      margin: 50px 0;
      background: #000000;
    }

    .video-container iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    footer {
      font-size: 0.875rem;
      color: #666666;
      margin: 50px 0 20px 0;
    }

    .video-material {
      text-align: center;
      padding: 40px 0;
    }

    .video-material h2 {
      color: #3498db;
      margin-bottom: 30px;
    }

    .video-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 30px;
      margin: 0 auto;
      max-width: 1200px;
    }
  </style>
</head>
<body>
  <!-- Title Banner -->
  <div class="title-banner">
    <h1 class="main-title">
      Q-DRIVE: Quadrupedal Deep Reinforcement for<br>
      Intelligent Versatile Execution
    </h1>
  </div>

  <div class="container">
    <!-- Authors -->
    <p class="authors">
      <span>Yu-Cheng Chang <sup>*1</sup></span>
      <span>Ching-Hsiang Wu <sup>*2</sup></span>
      <span>Yu-Chien Huang <sup>*3</sup></span>
      <span>Yu-Cheng Chang <sup>*4</sup></span><br>
      <span class="affiliation-note">
        1Simon Fraser University &nbsp;&middot;&nbsp; 2UIUC &nbsp;&middot;&nbsp; 3UC Berkeley &nbsp;&middot;&nbsp; 4Stanford &nbsp;&middot;&nbsp; ...
      </span>
      <span class="affiliation-note">
        *Equal Contribution
      </span>
    </p>

    <!-- Icons row -->
    <div class="icons-row">
      <div class="icon-wrapper">
        <a href="#" target="_blank">
          <img src="https://img.icons8.com/ios-filled/50/000000/document.png" alt="Paper Icon" />
          Paper
        </a>
      </div>
      <div class="icon-wrapper">
        <a href="#" target="_blank">
          <img src="https://img.icons8.com/ios-filled/50/000000/github.png" alt="GitHub Icon" />
          Code
        </a>
      </div>
      <div class="icon-wrapper">
        <a href="#" target="_blank">
          <img src="https://img.icons8.com/ios-filled/50/000000/youtube-play.png" alt="YouTube Icon" />
          Video
        </a>
      </div>
    </div>

    <!-- Abstract Section -->
    <h2 class="section-title">Abstract</h2>
    <p class="content-text">
      Reinforcement learning combined with sim-to-real transfer offers a general
      framework for developing locomotion controllers for legged robots. To
      facilitate successful deployment in the real world, smoothing techniques,
      such as low-pass filters and smoothness rewards, are often employed to
      develop policies with smooth behaviors. However, because these techniques
      are non-differentiable and usually require tedious tuning of many
      hyperparameters, they tend to require extensive manual tuning for each
      robotic platform. To address this challenge and establish a general
      technique for enforcing smooth behaviors, we propose a simple and
      effective method that imposes a Lipschitz constraint on a learned policy,
      which we refer to as Lipschitz-Constrained Policies (LCP). We show that
      the Lipschitz constraint can be implemented in the form of a gradient
      penalty, which provides a differentiable objective that can be easily
      incorporated with automatic differentiation frameworks. We demonstrate
      that LCP effectively replaces the need for smoothing rewards or low-pass
      filters and can be easily integrated into training frameworks for many
      distinct humanoid robots. We extensively evaluate LCP in both simulation
      and real-world humanoid robots, producing smooth and robust locomotion
      controllers.
    </p>

    <!-- Method Section -->
    <h2 class="section-title">Method</h2>
    <p class="content-text">
      RL-based policies are prone to producing <strong>jittery behaviors</strong>. 
      Lipschitz continuity is a way to characterize the smoothness of a function. 
      We propose Lipschitz-Constrained-Policies (LCP), a simple method to train policies 
      that produce smooth behaviors by enforcing a Lipschitz constraint on the policy. 
      This constraint is implemented as a gradient penalty, which is differentiable and 
      can be easily integrated into existing RL training pipelines with only a few lines 
      of code. LCP provides a simple and effective alternative to commonly used 
      non-differentiable smoothing techniques.
    </p>

    <!-- Images Section -->
    <div class="images-section">
      <img src="./rang3.png" alt="Sample Image 1">
      <img src="./test.png" alt="Sample Image 2">
      <img src="./test.png" alt="Sample Image 3">
    </div>

    <!-- Video Material Section -->
    <h2 class="section-title">Video Material</h2>
    <div class="video-grid">
      <div class="video-container">
        <iframe 
          src="https://www.youtube.com/watch?v=ofGrX75JnSU" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen>
        </iframe>
      </div>
      <div class="video-container">
        <iframe 
          src="https://www.youtube.com/watch?v=ofGrX75JnSU" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen>
        </iframe>
      </div>
    </div>

    <!-- Footer -->
    <footer>
      Â© 2024 Robotics Systems Lab
    </footer>
  </div>
</body>
</html>
